#!/usr/bin/env python3
"""
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†æãƒ„ãƒ¼ãƒ«
Markdownãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé–“ã®ãƒªãƒ³ã‚¯é–¢ä¿‚ã‚’åˆ†æã—ã¦ã€å­¤ç«‹ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¦‹ã¤ã‘ã‚‹
"""
import re
import hashlib
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
import click
from rich.console import Console
from rich.table import Table
from rich.tree import Tree


class DocumentAnalyzer:
    def __init__(self, root_path: Path):
        self.root_path = root_path
        self.documents: Dict[str, Path] = {}
        self.links: Dict[str, Set[str]] = {}
        self.incoming_links: Dict[str, Set[str]] = {}
        self.broken_links: Dict[str, List[Tuple[str, str]]] = {}  # doc_path -> [(link_text, link_url)]
        self.document_hashes: Dict[str, str] = {}
        self.duplicates: Dict[str, List[str]] = {}
        
    def scan_documents(self) -> None:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¹ã‚­ãƒ£ãƒ³"""
        for md_file in self.root_path.rglob("*.md"):
            # tools/node_modules/.venvãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯é™¤å¤–
            if "node_modules" in md_file.parts or ".venv" in md_file.parts:
                continue
            relative_path = md_file.relative_to(self.root_path)
            self.documents[str(relative_path)] = md_file
            self.links[str(relative_path)] = set()
            self.incoming_links[str(relative_path)] = set()
            self.broken_links[str(relative_path)] = []
    
    def calculate_document_hashes(self) -> None:
        """å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ã—ã¦é‡è¤‡ã‚’æ¤œå‡º"""
        hash_to_docs: Dict[str, List[str]] = {}
        
        for doc_path, doc_file in self.documents.items():
            try:
                content = doc_file.read_text(encoding='utf-8')
                # æ”¹è¡Œã‚„ã‚¹ãƒšãƒ¼ã‚¹ã‚’æ­£è¦åŒ–ã—ã¦ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—
                normalized_content = '\n'.join(line.strip() for line in content.splitlines() if line.strip())
                file_hash = hashlib.md5(normalized_content.encode()).hexdigest()
                
                self.document_hashes[doc_path] = file_hash
                
                if file_hash in hash_to_docs:
                    hash_to_docs[file_hash].append(doc_path)
                else:
                    hash_to_docs[file_hash] = [doc_path]
            except Exception:
                pass
        
        # é‡è¤‡ã‚’è¦‹ã¤ã‘ã‚‹
        self.duplicates = {hash_val: docs for hash_val, docs in hash_to_docs.items() if len(docs) > 1}
    
    def extract_links(self) -> None:
        """å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        link_pattern = re.compile(r'\[([^\]]+)\]\(([^)]+)\)')
        
        for doc_path, doc_file in self.documents.items():
            try:
                content = doc_file.read_text(encoding='utf-8')
                matches = link_pattern.findall(content)
                
                for link_text, link in matches:
                    # å¤–éƒ¨ãƒªãƒ³ã‚¯ã¯é™¤å¤–
                    if link.startswith('http://') or link.startswith('https://'):
                        continue
                    
                    # ã‚¢ãƒ³ã‚«ãƒ¼ãƒªãƒ³ã‚¯ã¯é™¤å¤–
                    if link.startswith('#'):
                        continue
                    
                    # ç›¸å¯¾ãƒ‘ã‚¹ã‚’è§£æ±º
                    if link.endswith('.md'):
                        target_path = self._resolve_link(doc_file, link)
                        if target_path:
                            self.links[doc_path].add(target_path)
                            self.incoming_links[target_path].add(doc_path)
                        else:
                            # ãƒªãƒ³ã‚¯åˆ‡ã‚Œã‚’è¨˜éŒ²
                            self.broken_links[doc_path].append((link_text, link))
            except Exception as e:
                print(f"Error reading {doc_path}: {e}")
    
    def _resolve_link(self, source_file: Path, link: str) -> str:
        """ç›¸å¯¾ãƒªãƒ³ã‚¯ã‚’è§£æ±º"""
        try:
            # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ã«è§£æ±º
            source_dir = source_file.parent
            target_path = (source_dir / link).resolve().relative_to(self.root_path)
            
            # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            if str(target_path) in self.documents:
                return str(target_path)
        except Exception:
            pass
        return None
    
    def find_isolated_documents(self) -> List[str]:
        """å­¤ç«‹ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¦‹ã¤ã‘ã‚‹"""
        isolated = []
        for doc_path in self.documents:
            # CLAUDE.mdã¨README.mdã¯ç‰¹åˆ¥æ‰±ã„ï¼ˆãƒ«ãƒ¼ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰
            if doc_path in ['CLAUDE.md', 'README.md']:
                continue
            
            # å…¥ã£ã¦ãã‚‹ãƒªãƒ³ã‚¯ãŒãªãã€å‡ºã¦ã„ããƒªãƒ³ã‚¯ã‚‚ãªã„
            if not self.incoming_links.get(doc_path) and not self.links.get(doc_path):
                isolated.append(doc_path)
        
        return sorted(isolated)
    
    def get_statistics(self) -> Dict[str, int]:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        total_links = sum(len(links) for links in self.links.values())
        total_broken = sum(len(broken) for broken in self.broken_links.values())
        
        # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã®ç·æ•°ã‚’è¨ˆç®—
        duplicate_count = sum(len(docs) - 1 for docs in self.duplicates.values())
        
        return {
            'total_documents': len(self.documents),
            'total_links': total_links,
            'isolated_documents': len(self.find_isolated_documents()),
            'documents_with_no_incoming': len([d for d in self.documents if not self.incoming_links.get(d)]),
            'documents_with_no_outgoing': len([d for d in self.documents if not self.links.get(d)]),
            'broken_links': total_broken,
            'duplicate_documents': duplicate_count
        }
    
    def build_tree_structure(self) -> Dict:
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‹ã‚‰ãƒ„ãƒªãƒ¼ã‚’æ§‹ç¯‰"""
        tree = {}
        
        for doc_path in self.documents:
            path = Path(doc_path)
            parts = path.parts
            
            current = tree
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéšå±¤ã‚’è¾¿ã‚‹
            for part in parts[:-1]:  # ãƒ•ã‚¡ã‚¤ãƒ«åä»¥å¤–
                if part not in current:
                    current[part] = {'type': 'dir', 'children': {}}
                current = current[part]['children']
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
            filename = parts[-1]
            current[filename] = {
                'type': 'file',
                'path': doc_path,
                'isolated': doc_path in self.find_isolated_documents()
            }
        
        return tree
    
    def generate_plantuml_mindmap(self, output_path: str = "document_mindmap.puml") -> str:
        """PlantUMLãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—å½¢å¼ã§ãƒ„ãƒªãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            ä¿å­˜ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        tree = self.build_tree_structure()
        
        plantuml_lines = [
            "@startmindmap",
            "!define PLANTUML_LIMIT_SIZE 16384",
            "skinparam dpi 300", 
            "skinparam defaultFontSize 12",
            "skinparam minClassWidth 50",
            "* Document Root",
        ]
        
        def traverse(node_dict, depth=1):
            indent = "*" * (depth + 1)
            
            for name, info in node_dict.items():
                display_name = name.replace('.md', '').replace('-', '_')
                
                if info['type'] == 'dir':
                    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆé’è‰²ï¼‰
                    plantuml_lines.append(f"{indent} ğŸ“ {display_name}")
                    traverse(info['children'], depth + 1)
                else:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã®è‰²åˆ†ã‘åˆ¤å®š
                    doc_path = info['path']
                    has_broken_links = len(self.broken_links.get(doc_path, [])) > 0
                    is_isolated = info['isolated']
                    incoming_count = len(self.incoming_links.get(doc_path, set()))
                    outgoing_count = len(self.links.get(doc_path, set()))
                    
                    if is_isolated:
                        # å­¤ç«‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆèµ¤è‰²ï¼‰
                        color = "[#red]"
                        suffix = " (ISOLATED)"
                    elif has_broken_links:
                        # å£Šã‚ŒãŸãƒªãƒ³ã‚¯ã‚ã‚Šï¼ˆã‚ªãƒ¬ãƒ³ã‚¸è‰²ï¼‰
                        color = "[#orange]"
                        suffix = " (BROKEN LINKS)"
                    elif incoming_count >= 5 or outgoing_count >= 8:
                        # é«˜ãƒªãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆç´«è‰²ï¼‰
                        color = "[#Plum]"
                        suffix = " (HIGH LINKS)"
                    else:
                        # é€šå¸¸ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆç·‘è‰²ï¼‰
                        color = "[#lightgreen]"
                        suffix = ""
                    
                    plantuml_lines.append(f"{indent}{color} {display_name}{suffix}")
        
        traverse(tree)
        
        plantuml_lines.append("@endmindmap")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_file = Path(output_path)
        output_file.write_text('\n'.join(plantuml_lines), encoding='utf-8')
        
        return str(output_file)


def display_results(analyzer: DocumentAnalyzer, console: Console):
    """çµæœã‚’è¡¨ç¤º"""
    # çµ±è¨ˆæƒ…å ±
    stats = analyzer.get_statistics()
    
    # çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
    stats_table = Table(title="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆçµ±è¨ˆ", show_header=True, header_style="bold magenta")
    stats_table.add_column("é …ç›®", style="cyan")
    stats_table.add_column("æ•°", justify="right", style="green")
    
    stats_table.add_row("ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°", str(stats['total_documents']))
    stats_table.add_row("ç·ãƒªãƒ³ã‚¯æ•°", str(stats['total_links']))
    stats_table.add_row("å­¤ç«‹ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ", str(stats['isolated_documents']))
    stats_table.add_row("å…¥åŠ›ãƒªãƒ³ã‚¯ãªã—", str(stats['documents_with_no_incoming']))
    stats_table.add_row("å‡ºåŠ›ãƒªãƒ³ã‚¯ãªã—", str(stats['documents_with_no_outgoing']))
    stats_table.add_row("å£Šã‚ŒãŸãƒªãƒ³ã‚¯", str(stats['broken_links']))
    stats_table.add_row("é‡è¤‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ", str(stats['duplicate_documents']))
    
    console.print(stats_table)
    console.print()
    
    # å­¤ç«‹ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    isolated = analyzer.find_isolated_documents()
    if isolated:
        console.print("[bold red]å­¤ç«‹ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:[/bold red]")
        for doc in isolated:
            console.print(f"  â€¢ {doc}")
        console.print()
    
    # å£Šã‚ŒãŸãƒªãƒ³ã‚¯
    has_broken_links = False
    for doc_path, broken_links in analyzer.broken_links.items():
        if broken_links:
            has_broken_links = True
            break
    
    if has_broken_links:
        console.print("[bold red]å£Šã‚ŒãŸãƒªãƒ³ã‚¯:[/bold red]")
        for doc_path, broken_links in sorted(analyzer.broken_links.items()):
            if broken_links:
                console.print(f"  [cyan]{doc_path}[/cyan]")
                for link_text, link_url in broken_links:
                    console.print(f"    â€¢ [{link_text}] â†’ {link_url}")
        console.print()
    
    # é‡è¤‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    if analyzer.duplicates:
        console.print("[bold yellow]é‡è¤‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:[/bold yellow]")
        for hash_val, docs in analyzer.duplicates.items():
            console.print(f"  ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å†…å®¹ãŒåŒä¸€ã§ã™:")
            for doc in sorted(docs):
                console.print(f"    â€¢ {doc}")
        console.print()
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã®ãƒªãƒ³ã‚¯æƒ…å ±
    docs_table = Table(title="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ¥ãƒªãƒ³ã‚¯æƒ…å ±", show_header=True, header_style="bold blue")
    docs_table.add_column("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ", style="cyan")
    docs_table.add_column("å‡ºåŠ›ãƒªãƒ³ã‚¯", justify="right", style="yellow")
    docs_table.add_column("å…¥åŠ›ãƒªãƒ³ã‚¯", justify="right", style="green")
    
    for doc_path in sorted(analyzer.documents.keys()):
        out_count = len(analyzer.links.get(doc_path, set()))
        in_count = len(analyzer.incoming_links.get(doc_path, set()))
        docs_table.add_row(doc_path, str(out_count), str(in_count))
    
    console.print(docs_table)


@click.command()
@click.option('--path', '-p', default=None, help='åˆ†æã™ã‚‹ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹')
@click.option('--verbose', '-v', is_flag=True, help='è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º')
@click.option('--graph', '-g', is_flag=True, help='ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã¦è¡¨ç¤º')
@click.option('--mindmap', '-m', is_flag=True, help='PlantUMLãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—ã‚’ç”Ÿæˆ')
@click.option('--discord', '-d', type=str, help='ã‚°ãƒ©ãƒ•ã‚’Discordãƒãƒ£ãƒ³ãƒãƒ«ã«é€ä¿¡ (ãƒãƒ£ãƒ³ãƒãƒ«ID)')
def analyze(path: str, verbose: bool, graph: bool, mindmap: bool, discord: Optional[str]):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªãƒ³ã‚¯é–¢ä¿‚ã‚’åˆ†æ"""
    console = Console()
    
    if path is None:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆï¼ˆ3éšå±¤ä¸Šï¼‰
        root_path = Path(__file__).resolve().parent.parent.parent
    else:
        root_path = Path(path).resolve()
    console.print(f"[bold green]åˆ†æå¯¾è±¡:[/bold green] {root_path}")
    console.print()
    
    analyzer = DocumentAnalyzer(root_path)
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¹ã‚­ãƒ£ãƒ³
    analyzer.scan_documents()
    console.print(f"[cyan]{len(analyzer.documents)}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç™ºè¦‹[/cyan]")
    
    # ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    analyzer.extract_links()
    
    # ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ã—ã¦é‡è¤‡ã‚’æ¤œå‡º
    analyzer.calculate_document_hashes()
    
    # çµæœã‚’è¡¨ç¤º
    display_results(analyzer, console)
    
    if verbose:
        # è©³ç´°ãªãƒªãƒ³ã‚¯æƒ…å ±
        console.print("\n[bold yellow]è©³ç´°ãªãƒªãƒ³ã‚¯æƒ…å ±:[/bold yellow]")
        for doc_path, links in analyzer.links.items():
            if links:
                console.print(f"\n{doc_path}:")
                for link in sorted(links):
                    console.print(f"  â†’ {link}")
    
    # ãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—ç”Ÿæˆ
    if mindmap:
        try:
            mindmap_path = analyzer.generate_plantuml_mindmap()
            console.print(f"\n[bold green]PlantUMLãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—ã‚’ç”Ÿæˆã—ã¾ã—ãŸ:[/bold green] {mindmap_path}")
            
            # PlantUMLã§ç”»åƒç”Ÿæˆã‚’è©¦ã™
            import subprocess
            try:
                result = subprocess.run(['plantuml', '-tpng', '-charset', 'UTF-8', 
                                       '-DPLANTUML_LIMIT_SIZE=16384', mindmap_path], 
                                      capture_output=True, text=True, timeout=30)
                if result.returncode == 0:
                    png_path = mindmap_path.replace('.puml', '.png')
                    console.print(f"[bold green]PNGç”»åƒã‚‚ç”Ÿæˆã—ã¾ã—ãŸ:[/bold green] {png_path}")
                else:
                    console.print(f"[yellow]PlantUMLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼:[/yellow] {result.stderr}")
            except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
                console.print(f"[yellow]PlantUMLã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚PUMLãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ç”Ÿæˆã—ã¾ã—ãŸ[/yellow]")
        except Exception as e:
            console.print(f"\n[bold red]ã‚¨ãƒ©ãƒ¼:[/bold red] ãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    
    # ã‚°ãƒ©ãƒ•ç”Ÿæˆ
    if graph or discord:
        try:
            from graph_visualizer import GraphVisualizer
            visualizer = GraphVisualizer(analyzer)
            
            # ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
            graph_path = visualizer.visualize()
            console.print(f"\n[bold green]ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã¾ã—ãŸ:[/bold green] {graph_path}")
            
            # Discordã«é€ä¿¡
            if discord:
                import os
                import sys
                # MCPãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã†ãŸã‚ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
                console.print(f"\n[bold blue]Discordãƒãƒ£ãƒ³ãƒãƒ« {discord} ã«é€ä¿¡ä¸­...[/bold blue]")
                # æ³¨: å®Ÿéš›ã®Discordé€ä¿¡ã¯MCPãƒ„ãƒ¼ãƒ«çµŒç”±ã§è¡Œã†å¿…è¦ãŒã‚ã‚‹
                console.print(f"[yellow]Discordé€ä¿¡æ©Ÿèƒ½ã¯åˆ¥é€”å®Ÿè£…ãŒå¿…è¦ã§ã™[/yellow]")
            
        except ImportError as e:
            console.print(f"\n[bold red]ã‚¨ãƒ©ãƒ¼:[/bold red] ã‚°ãƒ©ãƒ•ç”Ÿæˆã«å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“: {str(e)}")
            console.print(f"ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
            console.print(f"uv add networkx matplotlib numpy")
        except Exception as e:
            console.print(f"\n[bold red]ã‚¨ãƒ©ãƒ¼:[/bold red] ã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            import traceback
            traceback.print_exc()


if __name__ == '__main__':
    analyze()